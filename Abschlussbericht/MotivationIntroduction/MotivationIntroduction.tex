\section{Motivation und Einleitung}
Im Bereich des wissenschaftlichen Hochleistungsrechnen gibt es seit 1993 die
\glqq TOP500 List\grqq \cite{Top500}, welche die Supercomputer, geordnet nach ihrer Leistung 
in Floating Point Operationen pro Sekunde auflistet. Der momentane Spitzenreiter, 
der Tianhe-2, erreicht dabei bei voller Auslastung einen Energieverbrauch von 155.998,08 MWh 
jährlich. Dieser extrem hohe Wert bedeutet gleichzeitig große Betriebskosten,
 welche im Zeitalter der Energiewende kaum zu rechtfertigen sind. Daher wurde 
als Pendant im Jahr 2007 die \glqq Green500 List\grqq\, eröffnet, welche 
die Supercomputer entsprechend ihrer Rechenleistung pro Watt sortiert. Und genau 
an dieser setzt das Studienprojekt Technomathematik 2015/2016 der TU Dortmund an, 
welches begleitend zum ICARUS-Projekt des Lehrstuhl 3 der Fakultät Mathematik der TU Dortmund läuft. 
Ziel ist es, die Hardware- und Softwareentwicklung eines energieeffizienten 
\textit{High-Performance Computing} (HPC) Systems nachzuvollziehen und zu begleiten. 
Grundlage dafür ist der mobile Prozessor NVIDIA Tegra K1, 
welcher höchste Energieeffizienz verspricht. Die Nutzung dieses mobilen, 
auch eingebetteten Systems genannt (engl:\glqq embedded systems\grqq), 
erklärt sich durch die Anwendungsgebiete dieser Geräte. 
Sie werden in der Unterhaltungsindustrie, Automobilindustrie oder sogar als Herzschrittmacher in der Medizin eingesetzt. 
Diese Anwendungsbereiche verlangen ein enorm energiesparsames Verhalten, da die mobile Energiezufuhr über Batterien nicht endlos ist.
Daher sind diese Systeme eine attraktive Möglichkeit beim sogenannten \textit{Green Computing}. 
//
Das Hochleistungsrechnen spielt in der Mathematik eine große Rolle,
 da Differentialgleichungen Naturvorgänge verschiedener Art, wie zum Beispiel Schwingungen,
 Wellenausbreitungen oder Strömungen beschreiben können. Die Lösung dieser Gleichungen erfordert einen,
 je nach Genauigkeit des Resultats, erheblichen numerischen Aufwand,
 welcher nur mit Hilfe von geeigneter Hardware erreichbar ist. Da es das Vorhaben des Studienprojekts ist,
 den Luftstrom und die Wärmeleitung um und in einem geometrischen Körper wie dem des Tegra K1-Boards zu simulieren,
 spielen Differentialgleichungen eine wichtige Rolle für das Projekt. Außerdem ist es dafür wichtig,
 die Software nach dem Paradigma der hardware-orientierten Numerik zu konstruieren,
 um die Leistung des Grafikprozessors effizient auszunutzen. Dazu werden 60 der NVIDIA Boards zu drei Racks angeordnet,
 um den Code parallelisiert laufen lassen zu können. Man kann also im Umkehrschluss auch davon sprechen,
 dass die Hardware der Software angepasst wird, also \textit{software-orientierte Hardware}. 
//
Der Aufbau des Hauptteils des Berichts lässt sich dabei in drei große Themen gliedern.
 Zunächst die Hardware in seinen Einzelteilen, also die Hardwareauswahl,
 der Aufbau des Racks und die Energieverbrauchsmessung erklärt.
 Darauffolgend werden die Softwarekomponenten, welche die Assemblierung mit finiten Elementen und finiten Differenzen,
 sowie den Löser umfassen, erläutert. Abschließend werden verschiedene Testergebnisse,
 welche mit der Anordnung des Studienprojekts erzielt wurden, vorgestellt und die Rechenleistung anhand dieser Resultate bewertet. 



\section{Einleitung}


Im Bereich der hardware-orientierten Numerik, genannt textit{HWON}, unter der man die Abstimmung der Hardware Komponenten auf die bestmögliche Umsetzung der numerischen Methoden zur Handhabung einer Problemstellung versteht, wurden bereits in den vergangenen Jahren einige Untersuchungen gemacht. 
So wurde in den Jahren 2010 und 2011 durch Geveler et al. \cite{GevelerRibbrockMallachGoeddeke2011} die Lattice-Boltzmann-Methode zur Lösung von Navier-Stokes- und Flachwassergleichungen auf unterschiedlicher Hardware analysiert. Dabei stellte sich heraus, dass GPUs Multi-Core CPUs deutlich überlegen sind, ohne dabei an Genauigkeit der numerischen Lösung einzubü\ss en. 
In \glqq Efficient Finite Element Geometric Multigrid Solvers for Unstructured Grids on GPUs\grqq\cite{GevelerRibbrockGoeddekeZajacTurek2011}, wurde bereits 2011 nach dem Paradigma der hardware-orientierten Numerik ein geometrisches Mehrgitterverfahren mit finiter Elemente Assemblierung entwickelt, welches nur aus einer Reihe von Sparse Matrix-Vektor Multiplikationen besteht. Durch diese Implementierung, welche keinen Leistungsverlust nach sich zog, war es möglich, die Parallelität von Rechenarchitekturen noch besser auszunutzen. Besonders durch die Verwendung von GPUs statt Multi-Core CPUs ergab sich auch hier ein durchschnittlicher Speedup von acht.
Außerdem untersuchten Quintana et al. \cite{Quintana} 2013 das Zusammenwirken von Energieverbrauch und Rechenleistung anhand von effizient parallelisiertem Code auf Multi-Core Prozessoren. 
Geveler et al. betrachteten 2013, die Lösung partieller Differentialgleichungen mit finiten Elementen und Mehrgitterverfahren auf unstrukturierten Gittern \cite{GevelerRibbrockGoeddekeZajacTurek2011c}. Es wurde gezeigt, dass sich die Laufzeit der Anwendung erheblich reduzieren lässt, sobald GPUs anstatt von Multi-Core CPUs benutzt wurden. 
Zudem analysierten Benner et al. \cite{Benner} die Balance zwischen der Rechenleistung und dem Energieverbrauch von verschiedenen Mutli-Core Prozessoren anhand des Gau\ss schen Eliminationsverfahren zur Invertierung von Matrizen. Als Ergebnis zeigte sich unter anderem, dass eine Kombination aus einer GPU und einem energiesparsamen Multi-Core Prozessor einem gewöhnlichen Multi-Core Prozessor im Bereich der Energieeffizienz überlegen ist. 
Des Weiteren wurde in \glqq Energy efficieny vs. Performance of the numerical solution of PDEs: An application study on a low-power ARM-based cluster\grqq\cite{GoeddekeKomatitschGevelerRibbrockRajovicPuzovicRamirez2013}, ein Cluster aus 96 ARM Cortex-A9 Dual-Core Prozessoren einer Rechenarchitektur, basierend auf x86-Prozessoren gegenüber gestellt. Dabei wurde die verbrauchte Energie zur Lösung von drei wissenschaftlichen Anwendungen, einem Finite-Elemente Code mit Mehrgitter-Löser, einer strömungsmechanischen Anwendung und einem Code zur Ausbreitung von Schallwellen mit Hilfe der Spektral-Elemente Methode analysiert. Schließlich wurde gezeigt, dass die verbrauchte Energie zur Lösung erheblich gesenkt werden kann, im Einklang mit einer akzeptablen Erhöhung der Laufzeit. 
Hager et al. \cite{doi:10.1137/130930352} verdeutlichten 2015 anhand von verschiedenen Testmatrizen, dass das von ihnen implementierte Matrixformat, eine Variante des ELLPACK-Formats, hardware-unabhängig eine hohe Leistungseffizienz aufweist. 
In \glqq Porting FEASTFLOW to the Intel Xeon Phi: Lessons Learned\grqq\cite{VenetisGoumasGevelerRibbrock2015}, zeigten Geveler et al. die Leistungsverbesserung von axpy-Vektor Operationen und Sparse Matrix-Vektor Multiplikationen durch Nutzung des Intel Xeon Phi Koprozessors. Diese Analyse, sowie später durchgeführte Untersuchungen \cite{GevelerRibbrock2015}, welche sich mit der Entwicklung von numerischen Methoden zur parallelen Lösung von partiellen Differentialgleichungen für realitätsnahe industrielle und wissenschaftliche Probleme befassen, basierten auf der Software Infrastruktur \glqq FEASTFLOW\grqq. Dieses Paket umfasst Software zur numerischen Lösung der Navier-Stokes-Gleichungen in 2D und 3D. 
